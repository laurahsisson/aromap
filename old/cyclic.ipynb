{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0e367d5-50c2-43d7-a351-06988bd9a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import collections\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2622bc-7eb0-44cc-92ba-85a8ec35ceb8",
   "metadata": {},
   "source": [
    "According to Kohonen, we can do about 50 datapoints per node. For 3.5k that means maybe 60-80 nodes.\n",
    "I want a 5x4 resolution for display reasons, so maybe 10x8 nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7589844d-ba2c-4d78-8f78-08135bafe4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 19194/19194 [00:02<00:00, 6548.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3440 molecules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'amyl butyrate',\n",
       "  'smiles': 'CCCCCOC(=O)CCC',\n",
       "  'notes': {'banana', 'cherry', 'fruity', 'pineapple', 'sweet', 'tropical'}},\n",
       " {'name': 'para-anisyl phenyl acetate',\n",
       "  'smiles': 'COC1=CC=C(C=C1)COC(=O)CC2=CC=CC=C2',\n",
       "  'notes': {'anise', 'balsamic', 'honey', 'woody'}},\n",
       " {'name': 'dihydrofarnesol',\n",
       "  'smiles': 'CC(CC/C=C(\\\\C)/CCC=C(C)C)CCO',\n",
       "  'notes': {'cyclamen', 'floral', 'metallic'}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = os.listdir(\"data\")\n",
    "fields = ['name','smiles','notes']\n",
    "molecules = []\n",
    "for fname in tqdm.tqdm(fnames):\n",
    "    with open(os.path.join(\"data\",fname)) as f:\n",
    "        mol = json.load(f)\n",
    "        data = {k:mol[k] for k in fields}\n",
    "        # Check that the molecule has all the fields of interest\n",
    "        if all(data.values()):\n",
    "            molecules.append(data)\n",
    "\n",
    "# Have to de-dupe the notes for each molecule.\n",
    "for mol in molecules:\n",
    "    mol[\"notes\"] = set(mol[\"notes\"])\n",
    "    \n",
    "print(f\"Found {len(molecules)} molecules.\")\n",
    "molecules[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c1dbd5d-8a1f-470a-b30b-d96aafd9e33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 502 unique notes.\n",
      "Most common:\n",
      "[('fruity', 1060), ('green', 967), ('sweet', 884), ('floral', 706), ('woody', 558)]\n"
     ]
    }
   ],
   "source": [
    "all_notes = collections.Counter()\n",
    "for mol in molecules:\n",
    "    all_notes.update(mol[\"notes\"])\n",
    "print(f\"Found {len(all_notes)} unique notes.\")\n",
    "print(\"Most common:\")\n",
    "print(all_notes.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8870f225-50ef-4186-af00-d732f9734cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'amyl butyrate', 'smiles': 'CCCCCOC(=O)CCC', 'notes': {'banana', 'cherry', 'sweet', 'tropical', 'fruity', 'pineapple'}, 'encoding': tensor([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "all_notes_list = list(all_notes.keys())\n",
    "\n",
    "def multi_hot(notes):\n",
    "    indices = torch.tensor([all_notes_list.index(n) for n in notes])\n",
    "    if len(indices) == 0:\n",
    "        # Occurs when the notes in the pair were removed due to infrequency.\n",
    "        raise AttributeError(\"Found no valid notes.\")\n",
    "    one_hots = torch.nn.functional.one_hot(indices, len(all_notes))\n",
    "    return one_hots.sum(dim=0).float()\n",
    "\n",
    "for mol in molecules:\n",
    "    mol[\"encoding\"] = multi_hot(mol[\"notes\"])\n",
    "\n",
    "print(molecules[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1099b26-0506-43dc-baee-5d0716f70a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(mtrx):\n",
    "    return mtrx.reshape((mtrx.shape[0]*mtrx.shape[1],-1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9034fc8-b037-4531-9fbb-05e9ef6b8911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_grid(width,height,step):\n",
    "    x_idx, y_idx = torch.meshgrid(torch.arange(start=0,end=width-1+step,step=step), torch.arange(start=0,end=height-1+step,step=step), indexing='ij')\n",
    "    grid_idx = torch.stack([x_idx,y_idx],dim=-1)\n",
    "    return flatten(grid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c52f5b71-a336-4d9b-954c-f9ab113ce316",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 125\u001b[0m\n\u001b[1;32m    122\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([mol[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m mol \u001b[38;5;129;01min\u001b[39;00m molecules[:\u001b[38;5;241m10\u001b[39m]])\n\u001b[1;32m    123\u001b[0m     mm\u001b[38;5;241m.\u001b[39mupdate_batch(encodings)\n\u001b[0;32m--> 125\u001b[0m test()\n",
      "Cell \u001b[0;32mIn[9], line 121\u001b[0m, in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m():\n\u001b[0;32m--> 121\u001b[0m     mm \u001b[38;5;241m=\u001b[39m SOM(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    122\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([mol[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m mol \u001b[38;5;129;01min\u001b[39;00m molecules[:\u001b[38;5;241m10\u001b[39m]])\n\u001b[1;32m    123\u001b[0m     mm\u001b[38;5;241m.\u001b[39mupdate_batch(encodings)\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mSOM.__init__\u001b[0;34m(self, width, height, gauss, decay, use_onehot)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors \u001b[38;5;241m=\u001b[39m flatten(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_idx \u001b[38;5;241m=\u001b[39m get_idx_grid(width,height,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcydists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_cycle_distances()\n",
      "Cell \u001b[0;32mIn[9], line 67\u001b[0m, in \u001b[0;36mSOM.get_cycle_distances\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_cycle_distances\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# This is only computed once, so we could cache it if we wanted.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Distance from each node to every other node\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     eye \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 67\u001b[0m     flip_x \u001b[38;5;241m=\u001b[39m [w,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     68\u001b[0m     flip_y \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m,h]\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Results look better when map is on a torus, if we wanted a sphere this should be [w,h]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "class SOM(object):\n",
    "    def __init__(self,width,height,gauss=10,decay=.99,use_onehot=True):\n",
    "        if use_onehot:\n",
    "            # Select a random index to use as the hot element.\n",
    "            idxs = torch.randint(low=0,high=len(all_notes),size=(width,height))\n",
    "            # Convert to one hot of shape.\n",
    "            self.vectors = torch.nn.functional.one_hot(idxs,num_classes=len(all_notes)).float()\n",
    "        else:\n",
    "            self.vectors = torch.nn.functional.normalize(torch.rand(size=(width,height,len(all_notes))).float(),dim=-1)\n",
    "        \n",
    "        self.gauss = gauss\n",
    "        self.decay = decay\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        self.vectors = flatten(self.vectors)\n",
    "        self.map_idx = get_idx_grid(width,height,1)\n",
    "        self.cydists = self.get_cycle_distances()\n",
    "\n",
    "    def do_decay(self):\n",
    "        # From the author:\n",
    "        # For large SOMs, the final value of σ may be on the order of five per cent of the shorter side of the grid.\n",
    "        # For smaller ones, idk.\n",
    "        min_gauss = min(self.width,self.height)*.05\n",
    "        self.gauss *= self.decay\n",
    "        self.gauss = max(self.gauss,min_gauss)\n",
    "\n",
    "    def get_activations(self,encoding):\n",
    "        # Activation is 1 / Euclidian(vectors, encoding).\n",
    "        # The closer a vector is to the encoding, the higher the activation.\n",
    "        return 1/(self.vectors-encoding).square().sum(dim=-1).sqrt()\n",
    "\n",
    "    def get_enc_dists(self,encoding):\n",
    "        # Activation is 1 / Euclidian(vectors, encoding).\n",
    "        # The closer a vector is to the encoding, the higher the activation.\n",
    "        return (self.vectors-encoding).square().sum(dim=-1).sqrt()\n",
    "\n",
    "    def get_bmu(self,encoding):\n",
    "        actvtn = self.get_activations(encoding)\n",
    "        # Especially at the beginning of training, there may be a larger amount\n",
    "        # of vectors that are equidistant to the encoding. \n",
    "        bmu_idxs = (actvtn==torch.max(actvtn)).nonzero()\n",
    "        # In order to prevent embedding collapse, we select one randomly as the bmu.\n",
    "        selected = np.random.randint(low=0,high=len(bmu_idxs))\n",
    "        # print(bmu_idxs,bmu_idxs[selected])\n",
    "        return bmu_idxs[selected]\n",
    "\n",
    "    def get_bmu_pos(self,encoding):\n",
    "        return self.map_idx[self.get_bmu(encoding)]\n",
    "\n",
    "    def mean_encoding_by_bmu(self,encodings,bmus):\n",
    "        # https://stackoverflow.com/questions/56154604/groupby-aggregate-mean-in-pytorch/56155805#56155805\n",
    "        M = torch.zeros(len(self.vectors), len(encodings))\n",
    "        M[bmus, torch.arange(len(encodings))] = 1\n",
    "        M = torch.nn.functional.normalize(M, p=1, dim=1)\n",
    "        return torch.mm(M, encodings)\n",
    "\n",
    "    def get_flat_distances(self):\n",
    "        # Distance from each node to every other node\n",
    "        xy_dist = self.map_idx.unsqueeze(0) - self.map_idx.unsqueeze(1)\n",
    "        return torch.sqrt(torch.sum(torch.square(xy_dist),dim=-1))\n",
    "\n",
    "    def get_cycle_distances(self):\n",
    "        # This is only computed once, so we could cache it if we wanted.\n",
    "        # Distance from each node to every other node\n",
    "        eye = [0,0]\n",
    "        flip_x = [w,0]\n",
    "        flip_y = [0,h]\n",
    "        # Results look better when map is on a torus, if we wanted a sphere this should be [w,h]\n",
    "        flip_xy = eye\n",
    "        dist_all = []\n",
    "        for f in [eye,flip_x,flip_y,flip_xy]:\n",
    "            for sgn in [1,-1]:\n",
    "                transform_idx = self.map_idx + sgn*torch.tensor(f)\n",
    "                xy_dist = self.map_idx.unsqueeze(0) - transform_idx.unsqueeze(1)\n",
    "                dist_all.append(torch.sqrt(torch.sum(torch.square(xy_dist),dim=-1)))\n",
    "    \n",
    "        return torch.stack(dist_all).amin(0)\n",
    "\n",
    "    def get_distances(self):\n",
    "        return self.cydists\n",
    "\n",
    "    def get_intra_vector_dist_range(self):\n",
    "        xy_dist = self.vectors.unsqueeze(0) - self.vectors.unsqueeze(1)\n",
    "        dists = torch.sqrt(torch.sum(torch.square(xy_dist),dim=-1))\n",
    "        # We want to get the minimum distance from a vector to any OTHER vector,\n",
    "        # so we need to ignore the 0 distances (which are from a vector to itself).\n",
    "        return torch.min(dists[dists>0]), torch.max(dists)\n",
    "    \n",
    "    def get_intra_vector_activation_range(self):\n",
    "        xy_dist = self.vectors.unsqueeze(0) - self.vectors.unsqueeze(1)\n",
    "        dists = torch.sqrt(torch.sum(torch.square(xy_dist),dim=-1))\n",
    "        # We want to get the minimum distance from a vector to any OTHER vector,\n",
    "        # so we need to ignore the 0 distances (which are from a vector to itself).\n",
    "        # Activations are just inverse of distance.\n",
    "        return 1/torch.max(dists), 1/torch.min(dists[dists>0])\n",
    "\n",
    "    def update_factor(self):\n",
    "        dists = self.get_distances()\n",
    "        return torch.exp(torch.neg(torch.div(dists.square(), 2*self.gauss**2)))\n",
    "\n",
    "    def batch_sum_encodings(self,bmus,encodings):\n",
    "        # Although this is referred to as h_ji in the paper\n",
    "        # it is symmetric (so h[j][i] == h[i][j])\n",
    "        h_ij = self.update_factor()\n",
    "        x_mj = self.mean_encoding_by_bmu(encodings,bmus)\n",
    "        \n",
    "        bmu_count_by_idx = torch.bincount(bmus, minlength=len(self.map_idx))\n",
    "        # Unsqueeze the first dimension of the counts so that the update factor\n",
    "        # for i to j is weighted based on occurences of j.\n",
    "        weighted_h_ji = bmu_count_by_idx.unsqueeze(0)*h_ij\n",
    "\n",
    "        return torch.mm(weighted_h_ji, x_mj)/weighted_h_ji.sum(dim=-1,keepdim=True)\n",
    "\n",
    "    def update_batch(self,encodings):\n",
    "        # This step is not vectorized, but we could do a random partitioning or something above.\n",
    "        bmus = torch.cat([self.get_bmu(e) for e in encodings])\n",
    "        self.vectors = torch.nn.functional.normalize(self.batch_sum_encodings(bmus,encodings),dim=-1)\n",
    "        \n",
    "def test():\n",
    "    mm = SOM(3,2)\n",
    "    encodings = torch.stack([mol[\"encoding\"] for mol in molecules[:10]])\n",
    "    mm.update_batch(encodings)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48648f62-de8c-4bde-90cb-6151dcf38d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SOM__(object):\n",
    "#     def __init__(self,som):\n",
    "#         self.vectors = som.vectors\n",
    "#         self.map_idx = som.map_idx\n",
    "#         self.gauss = som.gauss\n",
    "\n",
    "#     def get_activations(self,encoding):\n",
    "#         # Activation is 1 / Euclidian(vectors, encoding).\n",
    "#         # The closer a vector is to the encoding, the higher the activation.\n",
    "#         return 1/(self.vectors-encoding).square().sum(dim=-1).sqrt()\n",
    "\n",
    "#     def get_bmu(self,encoding):\n",
    "#         actvtn = self.get_activations(encoding)\n",
    "#         # Especially at the beginning of training, there may be a larger amount\n",
    "#         # of vectors that are equidistant to the encoding. \n",
    "#         bmu_idxs = (actvtn==torch.max(actvtn)).nonzero()\n",
    "#         # In order to prevent embedding collapse, we select one randomly as the bmu.\n",
    "#         selected = np.random.randint(low=0,high=len(bmu_idxs))\n",
    "#         return bmu_idxs[selected]\n",
    "\n",
    "#     def mean_encoding_by_bmu__(self,encodings,bmus):\n",
    "#         sum_mj = torch.zeros(self.vectors.shape)\n",
    "#         count_mj = torch.zeros(self.vectors.shape[0])\n",
    "#         for i, v_idx in enumerate(bmus):\n",
    "#             count_mj[v_idx] += 1\n",
    "#             sum_mj[v_idx] += encodings[i]\n",
    "\n",
    "#         x_mj = torch.zeros(self.vectors.shape)\n",
    "#         for i, sm in enumerate(sum_mj):\n",
    "#             if count_mj[i] > 0:\n",
    "#                 x_mj[i] = sm / count_mj[i]\n",
    "#             else:\n",
    "#                 x_mj[i] = torch.zeros(sm.shape)\n",
    "\n",
    "#         return x_mj\n",
    "\n",
    "#     def update_factor__(self):\n",
    "#         uf = torch.empty((len(self.map_idx),len(self.map_idx)))\n",
    "#         for i, p1 in enumerate(self.map_idx):\n",
    "#             for j, p2 in enumerate(self.map_idx):\n",
    "#                 xy_dist = p1 - p2\n",
    "#                 d = torch.sqrt(torch.sum(torch.square(xy_dist),dim=-1))\n",
    "#                 uf[i][j] = torch.exp(torch.neg(torch.div(d.square(), 2*self.gauss**2)))\n",
    "#         return uf\n",
    "\n",
    "#     def batch_sum_encodings__(self,bmus,encodings):\n",
    "#         h_ij = self.update_factor__()\n",
    "#         x_mj = self.mean_encoding_by_bmu__(encodings,bmus)\n",
    "\n",
    "#         bmu_count_by_idx = torch.zeros(self.vectors.shape[0])\n",
    "#         for i, v_idx in enumerate(bmus):\n",
    "#             bmu_count_by_idx[v_idx] += 1\n",
    "\n",
    "#         bse = torch.zeros(self.vectors.shape)\n",
    "#         for i in range(len(h_ij)):\n",
    "#             denom = torch.zeros(bmu_count_by_idx.shape)\n",
    "#             for j in range(len(h_ij)):\n",
    "#                 bse[i] += x_mj[j] * h_ij[i][j] * bmu_count_by_idx[j]\n",
    "#                 denom[i] += h_ij[i][j] * bmu_count_by_idx[j]\n",
    "            \n",
    "#             if denom[i] > 0:\n",
    "#                 bse[i] = bse[i] / denom[i]\n",
    "#             else:\n",
    "#                 bse[i] = torch.zeros(bse[i].shape)\n",
    "\n",
    "#         return bse\n",
    "            \n",
    "        \n",
    "# def test():\n",
    "#     mm = SOM(3,2)\n",
    "#     mm__ = SOM__(mm)\n",
    "#     encodings = torch.stack([mol[\"encoding\"] for mol in molecules[:10]])\n",
    "#     bmus = torch.cat([mm.get_bmu(e) for e in encodings])\n",
    "\n",
    "#     assert torch.all(mm.vectors==mm__.vectors)\n",
    "\n",
    "#     def test_update_factor():\n",
    "#         nonlocal mm, mm__\n",
    "#         h_ij = mm.update_factor()\n",
    "#         h_ij__ = mm__.update_factor__()\n",
    "#         assert torch.isclose(h_ij,h_ij__).all()\n",
    "\n",
    "#     def test_update_factor_symmetric():\n",
    "#         nonlocal mm, mm__, bmus\n",
    "#         h_ij__ = mm__.update_factor__()\n",
    "#         for i in range(len(h_ij__)):\n",
    "#             for j in range(len(h_ij__)):\n",
    "#                 assert h_ij__[i][j] == h_ij__[j][i]\n",
    "            \n",
    "#     def test_mean_encoding_by_bmu():\n",
    "#         nonlocal mm, mm__, bmus, encodings\n",
    "#         x_mj = mm.mean_encoding_by_bmu(encodings,bmus)\n",
    "#         x_mj__ = mm__.mean_encoding_by_bmu__(encodings,bmus)\n",
    "#         assert torch.isclose(x_mj,x_mj__).all()\n",
    "\n",
    "#     def test_batch_sum_encodings():\n",
    "#         nonlocal mm, mm__, bmus, encodings\n",
    "#         # bmus are selected randomly, so we inject them here\n",
    "#         bse = mm.batch_sum_encodings(bmus,encodings)\n",
    "#         bse__ = mm__.batch_sum_encodings__(bmus,encodings)\n",
    "#         assert torch.isclose(bse,bse__).all()\n",
    "    \n",
    "#     test_update_factor()\n",
    "#     test_update_factor_symmetric()\n",
    "#     test_mean_encoding_by_bmu()\n",
    "#     test_batch_sum_encodings()\n",
    "    \n",
    "\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa8379b-f5a9-4ed7-8791-a7a0fa9012aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(som,use_freq=False):\n",
    "    d = torch.zeros(len(som.map_idx))\n",
    "    for n, f in all_notes.most_common():\n",
    "        bmu = som.get_bmu(multi_hot([n]))\n",
    "        if use_freq:\n",
    "            d[bmu] += f\n",
    "        else:\n",
    "            d[bmu] += 1\n",
    "    return d\n",
    "\n",
    "def make_histogram(som,use_freq=False):\n",
    "    d = get_distribution(som,use_freq)\n",
    "    plt.stairs(d,fill=True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbfafdd-6adf-4666-8534-b572d9af995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(molecules)/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ee5a4-17dd-45d8-9027-397d206b5d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mols, test_mols = sklearn.model_selection.train_test_split(molecules)\n",
    "\n",
    "def do_train(config,verbose=False):\n",
    "    som = SOM(width=config[\"width\"],\n",
    "              height=config[\"height\"],\n",
    "              gauss=config[\"gauss\"],\n",
    "              decay=config[\"decay\"],\n",
    "              use_onehot=config[\"onehot\"])\n",
    "\n",
    "    def get_score():\n",
    "        return get_distribution(som,True).std()\n",
    "    \n",
    "    encodings = torch.stack([mol[\"encoding\"] for mol in molecules])\n",
    "    scores = [get_score()]\n",
    "\n",
    "    steps = int(config[\"width\"] * config[\"height\"])\n",
    "    for i in tqdm.tqdm(range(steps),smoothing=0, disable=not verbose):\n",
    "        som.update_batch(encodings)\n",
    "        som.do_decay()\n",
    "        scores.append(get_score())\n",
    "\n",
    "    return som, scores\n",
    "\n",
    "w = 5\n",
    "h = 4\n",
    "# In general, the larger the initial gauss, the longer the training will have to be\n",
    "# Though using a stronger decay reduces this.\n",
    "# A larger initial gauss does tend to lead to better distributions, though.\n",
    "som,scores = do_train({'width': w, 'height': h, \"gauss\":min(w,h)/2, \"decay\": .9, \"onehot\": True},verbose=True)\n",
    "note_encodings = torch.stack([multi_hot([n]) for n,f in all_notes.most_common()])\n",
    "all_activations = torch.stack([som.get_activations(enc) for enc in note_encodings])\n",
    "amin, amax = all_activations.min(), all_activations.max()\n",
    "\n",
    "# In general the decay doesn't seem to affect final score?\n",
    "# We just get there slightly faster. To get around this I'll used steps = w * h\n",
    "# I'll stick to decay = .9 for now (and NOT PLAY WITH IT)\n",
    "\n",
    "# In order to have any actual hparam tuning we'd need to just make a somewhat exhausting search.\n",
    "# Hparams I do not want to search\n",
    "# Steps (reaches a stable state after a while)\n",
    "# Decay (doesn't seem to effect the final result, just how long to get there)\n",
    "# For most gauss, steps = w*h and decay = 0.9 seems to eventually reach steady state.\n",
    "\n",
    "# Hparams I do want to search\n",
    "# W, H (fixed by suggestion of author, but I could change\n",
    "# gauss (I'd suggest something like tuning n where gauss = min(w,h)*n\n",
    "# onehot (But maybe doesn't really do that much)\n",
    "# Normalizing models/vectors (I think doing this makes a huge difference)\n",
    "# Min gauss ?\n",
    "\n",
    "# One issue is that bmu distribution of notes doesn't seem to capture everything?\n",
    "# Maybe distribution unweighted of molecules and notes?\n",
    "\n",
    "print(scores[-1],som.gauss)\n",
    "plt.plot(scores)\n",
    "plt.show()\n",
    "\n",
    "make_histogram(som,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1549291-602f-4518-8a70-13640c1f4b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, f in all_notes.most_common(15):\n",
    "    print(n,som.get_bmu_pos(multi_hot([n])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba89706-eba8-4a30-86d6-67e1cb820288",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mol in molecules[:15]:\n",
    "    print(mol[\"name\"],som.get_bmu_pos(mol[\"encoding\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619bb584-e314-442b-b116-044981d2c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(title,encoding,as_size,factor=1,thresh=1e-5,activations=None):\n",
    "    bmu = som.get_bmu(encoding)\n",
    "    if not torch.is_tensor(activations):\n",
    "        activations = som.get_activations(encoding)\n",
    "\n",
    "    act = (activations - amin) / (amax - amin)\n",
    "    minv, maxv = act.min().numpy(), act.max().numpy()\n",
    "    assert act[bmu].item() == maxv\n",
    "\n",
    "    pos = som.map_idx.numpy()\n",
    "    act = torch.nn.functional.threshold(act,thresh,0)\n",
    "    display_val = (act**2)\n",
    "    if as_size:\n",
    "        # For very very small values, matplotlib will underflow and draw circles where it should draw tiny circles.\n",
    "        print(display_val.min(),display_val.max())\n",
    "        plt.scatter(pos[:,0],pos[:,1],s=factor*display_val.numpy())\n",
    "    else:\n",
    "        plt.scatter(pos[:,0],pos[:,1],c=factor*display_val.numpy())\n",
    "        plt.set_cmap('PiYG_r')\n",
    "        plt.colorbar()\n",
    "\n",
    "    plt.title(f\"{title}\\nBMU of {bmu.numpy()} w/ value = {act[bmu].item():.2f}. Range = ({minv:.2f}, {maxv:.2f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b71db-c635-436e-bfae-442e1ca2b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(f\"Map for {molecules[0]['name']} w/ notes = {molecules[0]['notes']}\",\n",
    "     molecules[0][\"encoding\"],\n",
    "     as_size=True,\n",
    "     factor=1000,\n",
    "     thresh=1e-2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1719e-8b47-47f2-b038-7f81c76e3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_activation(step,activations,method=\"linear\"):\n",
    "    global w,h\n",
    "    fine_grid = get_idx_grid(w,h,step)\n",
    "    assert len(activations.shape) == 1\n",
    "    fine_act = scipy.interpolate.griddata(som.map_idx.numpy(),activations.numpy(),fine_grid.numpy(),method=method)\n",
    "    assert fine_act[0] == activations[0]\n",
    "    assert torch.all(fine_grid[0] == som.map_idx[0])\n",
    "    \n",
    "    assert fine_act[-1] == activations[-1]\n",
    "    assert torch.all(fine_grid[-1] == som.map_idx[-1])\n",
    "    \n",
    "    return fine_grid, torch.FloatTensor(fine_act)\n",
    "    \n",
    "fidx,fa = interpolate_activation(.1,all_activations[0])\n",
    "fidx.shape,fa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae5ae94-eb31-4bdc-9f5c-047b83606325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fine(title,encoding,as_size,step,interpolation,thresh=1e-5,activations=None):\n",
    "    bmu = som.get_bmu(encoding)\n",
    "    if not torch.is_tensor(activations):\n",
    "        activations = som.get_activations(encoding)\n",
    "\n",
    "    fpos, fact = interpolate_activation(step,activations,method=interpolation)\n",
    "    act = (fact - amin) / (amax - amin)\n",
    "    \n",
    "    minv, maxv = act.min().numpy(), act.max().numpy()\n",
    "    if interpolation != \"cubic\":\n",
    "        # For cubic interpolation, values higher than the previous peak may be created.\n",
    "        # For linear and nearest this will not happen and we can do this sanity check.\n",
    "        assert activations[bmu].item() == fact.max()\n",
    "\n",
    "    # Multiply by step^2 to account for width*height, then again to account for radius of points\n",
    "    factor = 1e7/(som.width*som.height*(1/(step**3))) \n",
    "    act = torch.nn.functional.threshold(act,thresh,0)\n",
    "    display_val = (act**2)\n",
    "    if as_size:\n",
    "        plt.scatter(fpos[:,0],fpos[:,1],s=factor*display_val.numpy())\n",
    "    else:\n",
    "        plt.scatter(fpos[:,0],fpos[:,1],c=factor*display_val.numpy())\n",
    "        plt.set_cmap('PiYG_r')\n",
    "        plt.colorbar()\n",
    "\n",
    "    bmu_act = (activations[bmu].item() - amin) / (amax - amin)\n",
    "    plt.title(f\"{title}\\nBMU of {som.get_bmu_pos(encoding)[0].numpy()} w/ value = {bmu_act:.2f}. Range = ({minv:.2f}, {maxv:.2f})\")\n",
    "\n",
    "plot_fine(f\"Map for {molecules[0]['name']} w/ notes = {molecules[0]['notes']}\",\n",
    "     molecules[0][\"encoding\"],\n",
    "     as_size=True,\n",
    "     thresh=1e-1,\n",
    "     interpolation=\"linear\",\n",
    "     step=.01)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c23d8e-5ed0-4364-a077-6c857fed96ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for interp in [\"linear\",\"cubic\",\"nearest\"]:\n",
    "    n = \"fruity\"\n",
    "    plot_fine(f\"{n} w/ {interp}\",multi_hot([n]),interpolation=interp,step=.01,as_size=True,thresh=0)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb17b8-00ff-4a89-8d9e-66010ba801ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this instead of a fixed threshold\n",
    "# Maybe .5?\n",
    "all_activations.quantile(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87387c14-bae6-457b-91dd-32f1295580e9",
   "metadata": {},
   "source": [
    "In my opinion, the color grading in linear is better, but the edges are smoother in cubic.\n",
    "So I'd like to fix color grading later, but edges are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f54eeb-1504-4ce4-b41c-1cc2fbca24c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, f in all_notes.most_common(15):\n",
    "    plot_fine(n,multi_hot([n]),interpolation=\"cubic\",step=.01,as_size=True,thresh=1-1)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb0289-f4af-4fa5-82d7-c4fff13df582",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, f in all_notes.most_common(3):\n",
    "    print(n,som.get_bmu_pos(multi_hot([n])))\n",
    "    plot_fine(n,multi_hot([n]),interpolation=\"cubic\",step=.01,as_size=True,thresh=5e-2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb37bf-b6cf-411e-b78b-2c9e3730f232",
   "metadata": {},
   "source": [
    "This doesn't look good because there is no blending, but amazing start."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
