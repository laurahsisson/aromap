{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0e367d5-50c2-43d7-a351-06988bd9a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import collections\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2622bc-7eb0-44cc-92ba-85a8ec35ceb8",
   "metadata": {},
   "source": [
    "According to Kohonen, we can do about 50 datapoints per node. For 3.5k that means maybe 60-80 nodes.\n",
    "I want a 5x4 resolution for display reasons, so maybe 10x8 nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7589844d-ba2c-4d78-8f78-08135bafe4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 19194/19194 [00:02<00:00, 6835.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3440 molecules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'amyl butyrate',\n",
       "  'smiles': 'CCCCCOC(=O)CCC',\n",
       "  'notes': {'banana', 'cherry', 'fruity', 'pineapple', 'sweet', 'tropical'}},\n",
       " {'name': 'para-anisyl phenyl acetate',\n",
       "  'smiles': 'COC1=CC=C(C=C1)COC(=O)CC2=CC=CC=C2',\n",
       "  'notes': {'anise', 'balsamic', 'honey', 'woody'}},\n",
       " {'name': 'dihydrofarnesol',\n",
       "  'smiles': 'CC(CC/C=C(\\\\C)/CCC=C(C)C)CCO',\n",
       "  'notes': {'cyclamen', 'floral', 'metallic'}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = os.listdir(\"data\")\n",
    "fields = ['name','smiles','notes']\n",
    "molecules = []\n",
    "for fname in tqdm.tqdm(fnames):\n",
    "    with open(os.path.join(\"data\",fname)) as f:\n",
    "        mol = json.load(f)\n",
    "        data = {k:mol[k] for k in fields}\n",
    "        # Check that the molecule has all the fields of interest\n",
    "        if all(data.values()):\n",
    "            molecules.append(data)\n",
    "\n",
    "# Have to de-dupe the notes for each molecule.\n",
    "for mol in molecules:\n",
    "    mol[\"notes\"] = set(mol[\"notes\"])\n",
    "    \n",
    "print(f\"Found {len(molecules)} molecules.\")\n",
    "molecules[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c1dbd5d-8a1f-470a-b30b-d96aafd9e33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 502 unique notes.\n",
      "Most common:\n",
      "[('fruity', 1060), ('green', 967), ('sweet', 884), ('floral', 706), ('woody', 558)]\n"
     ]
    }
   ],
   "source": [
    "all_notes = collections.Counter()\n",
    "for mol in molecules:\n",
    "    all_notes.update(mol[\"notes\"])\n",
    "print(f\"Found {len(all_notes)} unique notes.\")\n",
    "print(\"Most common:\")\n",
    "print(all_notes.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8870f225-50ef-4186-af00-d732f9734cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'amyl butyrate', 'smiles': 'CCCCCOC(=O)CCC', 'notes': {'cherry', 'fruity', 'banana', 'sweet', 'tropical', 'pineapple'}, 'encoding': tensor([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "all_notes_list = list(all_notes.keys())\n",
    "\n",
    "def multi_hot(notes):\n",
    "    indices = torch.tensor([all_notes_list.index(n) for n in notes])\n",
    "    if len(indices) == 0:\n",
    "        # Occurs when the notes in the pair were removed due to infrequency.\n",
    "        raise AttributeError(\"Found no valid notes.\")\n",
    "    one_hots = torch.nn.functional.one_hot(indices, len(all_notes))\n",
    "    return one_hots.sum(dim=0).float()\n",
    "\n",
    "for mol in molecules:\n",
    "    mol[\"encoding\"] = multi_hot(mol[\"notes\"])\n",
    "\n",
    "print(molecules[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "c52f5b71-a336-4d9b-954c-f9ab113ce316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "def flatten(mtrx):\n",
    "    return mtrx.reshape((mtrx.shape[0]*mtrx.shape[1],-1)).squeeze()\n",
    "\n",
    "class SOM(object):\n",
    "    def __init__(self,width,height,lr=1e-1,gauss=10,decay=.99,use_onehot=True):\n",
    "        if use_onehot:\n",
    "            # Select a random index to use as the hot element.\n",
    "            idxs = torch.randint(low=0,high=len(all_notes),size=(width,height))\n",
    "            # Convert to one hot of shape.\n",
    "            self.vectors = torch.nn.functional.one_hot(idxs,num_classes=len(all_notes)).float()\n",
    "        else:\n",
    "            self.vectors = torch.rand(size=(width,height,len(all_notes))).float()\n",
    "        \n",
    "        map_x_idx, map_y_idx = torch.meshgrid(torch.arange(start=0,end=width), torch.arange(start=0,end=height), indexing='ij')\n",
    "        self.map_idx = torch.stack([map_x_idx,map_y_idx],dim=-1)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.gauss = gauss\n",
    "        self.decay = decay\n",
    "\n",
    "        self.vectors = flatten(self.vectors)\n",
    "        self.map_idx = flatten(self.map_idx)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.lr\n",
    "\n",
    "    def do_decay(self):\n",
    "        self.lr *= self.decay\n",
    "        self.gauss *= self.decay\n",
    "\n",
    "    def get_activations(self,encoding):\n",
    "        # Activation is 1 / Euclidian(vectors, encoding).\n",
    "        # The closer a vector is to the encoding, the higher the activation.\n",
    "        return 1/(self.vectors-encoding).square().sum(dim=-1).sqrt()\n",
    "\n",
    "    def get_bmu(self,encoding):\n",
    "        actvtn = self.get_activations(encoding)\n",
    "        # Especially at the beginning of training, there may be a larger amount\n",
    "        # of vectors that are equidistant to the encoding. \n",
    "        bmu_idxs = (actvtn==torch.max(actvtn)).nonzero()\n",
    "        # In order to prevent embedding collapse, we select one randomly as the bmu.\n",
    "        selected = np.random.randint(low=0,high=len(bmu_idxs))\n",
    "        return bmu_idxs[selected]\n",
    "\n",
    "    def mean_encoding_by_bmu(self,encodings,bmus):\n",
    "        # https://stackoverflow.com/questions/56154604/groupby-aggregate-mean-in-pytorch/56155805#56155805\n",
    "        M = torch.zeros(len(self.vectors), len(encodings))\n",
    "        M[bmus, torch.arange(len(encodings))] = 1\n",
    "        M = torch.nn.functional.normalize(M, p=1, dim=1)\n",
    "        return torch.mm(M, encodings)\n",
    "\n",
    "    def get_distances(self):\n",
    "        # Distance from each node to every other node\n",
    "        xy_dist = self.map_idx.unsqueeze(0) - self.map_idx.unsqueeze(1)\n",
    "        return torch.sqrt(torch.sum(torch.square(xy_dist),dim=-1))\n",
    "\n",
    "    def update_factor(self,bmus):\n",
    "        dists = self.get_distances()\n",
    "        # Gaussian distribution centered on BMU of width 2^gauss.\n",
    "        h_ji = torch.exp(torch.neg(torch.div(dists.square(), self.gauss**2)))\n",
    "        bmu_count_by_idx = torch.bincount(bmus, minlength=len(self.map_idx))\n",
    "        # Unsqueeze the first dimension of the counts so that the update factor\n",
    "        # for i to j is weighted based on occurences of j.\n",
    "        return bmu_count_by_vector.unsqueeze(0)*h_ji\n",
    "        \n",
    "\n",
    "    def update_batch(self,encodings):\n",
    "        bmus = torch.cat([mm.get_bmu(e) for e in encodings])\n",
    "        # Although this is average by the number of bmus for each vector (model)\n",
    "        weighted_x_mj = self.mean_encoding_by_bmu(encodings,bmus)\n",
    "        # The update factor is not averaged in this way so we'd need to do something like in mean_encoding_by_bmu\n",
    "        h_ji = self.update_factor()\n",
    "        # self.mean_encoding_by_bmu(h_ji,bmus)\n",
    "        # There is definitely a cleaner way to do this\n",
    "        bmu_count_by_vector = torch.bincount(bmus, minlength=len(self.map_idx))\n",
    "        print(h_ji.shape)\n",
    "        print(bmu_count_by_vector.shape)\n",
    "        bmu_count_by_vector.unsqueeze(0)*h_ji\n",
    "        self.vectors = torch.mm(h_ji, x_mj)\n",
    "\n",
    "mm = SOM(3,2,lr=1)\n",
    "encodings = torch.stack([mol[\"encoding\"] for mol in molecules[:10]])\n",
    "mm.update_batch(encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9e2af2f3-7735-4304-a8c4-fa0a8d940e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f253c24d-d9e9-4180-bfb8-f6dc12215dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = SOM(3,2,lr=1)\n",
    "encodings = torch.stack([mol[\"encoding\"] for mol in molecules[:10]])\n",
    "bmus = torch.cat([mm.get_bmu(e) for e in encodings])\n",
    "bmu_idxs = mm.map_idx[bmus]\n",
    "\n",
    "x_mj = mm.mean_encoding_by_bmu(encodings,bmus)\n",
    "xy_dist = mm.map_idx.unsqueeze(0) - mm.map_idx.unsqueeze(1)\n",
    "dists = torch.sqrt(torch.sum(torch.square(xy_dist),dim=-1))\n",
    "# dists = torch.cdist(mm.map_idx.float(),bmu_idxs.float())\n",
    "# uf = torch.exp(torch.neg(torch.div(dists.square(), mm.gauss**2)))\n",
    "\n",
    "uf = torch.exp(torch.neg(torch.div(dists.square(), mm.gauss**2)))\n",
    "# We need to mean first\n",
    "res = uf @ x_mj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ee5a4-17dd-45d8-9027-397d206b5d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mols, test_mols = sklearn.model_selection.train_test_split(molecules)\n",
    "\n",
    "def do_train(config,verbose=False):\n",
    "    total_steps = config[\"bsz\"]*config[\"batches\"]\n",
    "    \n",
    "\n",
    "    som = SOM(width=config[\"width\"],\n",
    "              height=config[\"height\"],\n",
    "              lr=config[\"lr\"],\n",
    "              gauss=config[\"gauss\"],\n",
    "              decay=config[\"decay\"],\n",
    "              use_onehot=config[\"onehot\"])\n",
    "    \n",
    "    def do_train_step(s):\n",
    "        slr = config[\"lr\"]*(1 - (s/total_steps))\n",
    "        mol = random.choice(train_mols)\n",
    "        return som.update_step(mol[\"encoding\"])\n",
    "    \n",
    "    def get_test_loss():\n",
    "        es = []\n",
    "        for mol in test_mols:\n",
    "            es.append(som.get_loss(mol[\"encoding\"]))\n",
    "        return np.mean(es)\n",
    "\n",
    "    lrs = []\n",
    "    losses = []\n",
    "    for s in tqdm.tqdm(range(total_steps),smoothing=0, disable=not verbose):\n",
    "        do_train_step(s)\n",
    "        if s % config[\"bsz\"] == 0:\n",
    "            losses.append(get_test_loss())\n",
    "            som.do_decay()\n",
    "            lrs.append(som.get_lr())\n",
    "\n",
    "    return som, losses, lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0ea63-1891-4f3c-abf4-6e004baf447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "som, loss, lrs = do_train({'width': 5*2, 'height': 4*2, 'bsz': 2**8, 'batches': 500, 'lr': 1e-2, \"gauss\":5, \"decay\": .99, \"onehot\": True},verbose=True)\n",
    "plt.plot(lrs,loss)\n",
    "plt.yscale('log')\n",
    "# loss decreases over time, so plot by decreasing lr.\n",
    "plt.gca().invert_xaxis()\n",
    "for n, f in all_notes.most_common(10):\n",
    "    print(n,som.get_bmu(multi_hot([n])))\n",
    "plt.show()\n",
    "print(loss[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619bb584-e314-442b-b116-044981d2c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(mtrx):\n",
    "    return mtrx.reshape((mtrx.shape[0]*mtrx.shape[1],-1)).squeeze()\n",
    "\n",
    "def plot(title,encoding,as_size,factor=1,thresh=1e-5,activations=None):\n",
    "    bmu = som.get_bmu(encoding)\n",
    "    if not torch.is_tensor(activations):\n",
    "        activations = som.get_activations(encoding)\n",
    "    \n",
    "    act = flatten(activations)\n",
    "    minv, maxv = act.min().numpy(), act.max().numpy()\n",
    "    \n",
    "    pos = flatten(som.map_idx).numpy()\n",
    "    \n",
    "    if as_size:\n",
    "        # For very very small values, matplotlib will underflow and draw circles where it should draw tiny circles.\n",
    "        act = torch.nn.functional.threshold(act,thresh,0)\n",
    "        plt.scatter(pos[:,0],pos[:,1],s=factor*act.numpy())\n",
    "    else:\n",
    "        plt.scatter(pos[:,0],pos[:,1],c=factor*act.numpy())\n",
    "        plt.set_cmap('PiYG_r')\n",
    "        plt.colorbar()\n",
    "        # cbar.lim(minv,maxv)\n",
    "\n",
    "    plt.title(f\"{title}\\nBMU of {bmu.numpy()} w/ value = {activations[bmu[0],bmu[1]]}. Range = ({minv:.2f}, {maxv:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebce99a-2fc9-444f-8732-57cf4004129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(f\"Map for {molecules[0]['name']} w/ notes = {molecules[0]['notes']}\",\n",
    "     molecules[0][\"encoding\"],\n",
    "     as_size=True,\n",
    "     factor=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759684e6-c7fd-402f-877a-a86917d70955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(\"\",multi_hot([\"musk\"]),as_size=True,thresh=1e-1)\n",
    "# plot(\"\",multi_hot([\"sweet\"]),as_size=True,thresh=1e-1)\n",
    "# plot(\"\",multi_hot([\"sour\"]),as_size=True,thresh=1e-1)\n",
    "# plot(\"\",multi_hot([\"fruity\"]),as_size=True,thresh=1e-1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf8f70b-20a1-4872-9f01-8954e6f212a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(f\"Update factor for {molecules[0]['name']} w/ notes = {molecules[0]['notes']}\",\n",
    "     molecules[0][\"encoding\"],\n",
    "     as_size=True,\n",
    "     factor=30,\n",
    "     activations= som.update_factor(som.get_bmu(molecules[0][\"encoding\"])))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f54eeb-1504-4ce4-b41c-1cc2fbca24c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, f in all_notes.most_common(100):\n",
    "    # plot(n,multi_hot([n]),as_size=True,thresh=1e-1)\n",
    "    # plt.show()\n",
    "    print(n,som.get_bmu(multi_hot([n])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb0289-f4af-4fa5-82d7-c4fff13df582",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
