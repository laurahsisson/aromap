{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e367d5-50c2-43d7-a351-06988bd9a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import collections\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2622bc-7eb0-44cc-92ba-85a8ec35ceb8",
   "metadata": {},
   "source": [
    "According to Kohonen, we can do about 50 datapoints per node. For 3.5k that means maybe 60-80 nodes.\n",
    "I want a 5x4 resolution for display reasons, so maybe 10x8 nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7589844d-ba2c-4d78-8f78-08135bafe4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 19194/19194 [00:02<00:00, 6956.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3440 molecules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'amyl butyrate',\n",
       "  'smiles': 'CCCCCOC(=O)CCC',\n",
       "  'notes': {'banana', 'cherry', 'fruity', 'pineapple', 'sweet', 'tropical'}},\n",
       " {'name': 'para-anisyl phenyl acetate',\n",
       "  'smiles': 'COC1=CC=C(C=C1)COC(=O)CC2=CC=CC=C2',\n",
       "  'notes': {'anise', 'balsamic', 'honey', 'woody'}},\n",
       " {'name': 'dihydrofarnesol',\n",
       "  'smiles': 'CC(CC/C=C(\\\\C)/CCC=C(C)C)CCO',\n",
       "  'notes': {'cyclamen', 'floral', 'metallic'}}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = os.listdir(\"data\")\n",
    "fields = ['name','smiles','notes']\n",
    "molecules = []\n",
    "for fname in tqdm.tqdm(fnames):\n",
    "    with open(os.path.join(\"data\",fname)) as f:\n",
    "        mol = json.load(f)\n",
    "        data = {k:mol[k] for k in fields}\n",
    "        # Check that the molecule has all the fields of interest\n",
    "        if all(data.values()):\n",
    "            molecules.append(data)\n",
    "\n",
    "# Have to de-dupe the notes for each molecule.\n",
    "for mol in molecules:\n",
    "    mol[\"notes\"] = set(mol[\"notes\"])\n",
    "    \n",
    "print(f\"Found {len(molecules)} molecules.\")\n",
    "molecules[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c1dbd5d-8a1f-470a-b30b-d96aafd9e33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 502 unique notes.\n",
      "Most common:\n",
      "[('fruity', 1060), ('green', 967), ('sweet', 884), ('floral', 706), ('woody', 558)]\n"
     ]
    }
   ],
   "source": [
    "all_notes = collections.Counter()\n",
    "for mol in molecules:\n",
    "    all_notes.update(mol[\"notes\"])\n",
    "print(f\"Found {len(all_notes)} unique notes.\")\n",
    "print(\"Most common:\")\n",
    "print(all_notes.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8870f225-50ef-4186-af00-d732f9734cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'amyl butyrate', 'smiles': 'CCCCCOC(=O)CCC', 'notes': {'cherry', 'tropical', 'pineapple', 'fruity', 'sweet', 'banana'}, 'encoding': tensor([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "all_notes_list = list(all_notes.keys())\n",
    "\n",
    "def multi_hot(notes):\n",
    "    indices = torch.tensor([all_notes_list.index(n) for n in notes])\n",
    "    if len(indices) == 0:\n",
    "        # Occurs when the notes in the pair were removed due to infrequency.\n",
    "        raise AttributeError(\"Found no valid notes.\")\n",
    "    one_hots = torch.nn.functional.one_hot(indices, len(all_notes))\n",
    "    return one_hots.sum(dim=0).float()\n",
    "\n",
    "for mol in molecules:\n",
    "    mol[\"encoding\"] = multi_hot(mol[\"notes\"])\n",
    "\n",
    "print(molecules[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c52f5b71-a336-4d9b-954c-f9ab113ce316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(mtrx):\n",
    "    return mtrx.reshape((mtrx.shape[0]*mtrx.shape[1],-1)).squeeze()\n",
    "\n",
    "class SOM(object):\n",
    "    def __init__(self,width,height,gauss=10,decay=.99,use_onehot=True):\n",
    "        if use_onehot:\n",
    "            # Select a random index to use as the hot element.\n",
    "            idxs = torch.randint(low=0,high=len(all_notes),size=(width,height))\n",
    "            # Convert to one hot of shape.\n",
    "            self.vectors = torch.nn.functional.one_hot(idxs,num_classes=len(all_notes)).float()\n",
    "        else:\n",
    "            self.vectors = torch.rand(size=(width,height,len(all_notes))).float()\n",
    "        \n",
    "        map_x_idx, map_y_idx = torch.meshgrid(torch.arange(start=0,end=width), torch.arange(start=0,end=height), indexing='ij')\n",
    "        self.map_idx = torch.stack([map_x_idx,map_y_idx],dim=-1)\n",
    "\n",
    "        self.gauss = gauss\n",
    "        self.decay = decay\n",
    "\n",
    "        self.vectors = flatten(self.vectors)\n",
    "        self.map_idx = flatten(self.map_idx)\n",
    "\n",
    "    def do_decay(self):\n",
    "        self.gauss *= self.decay\n",
    "\n",
    "    def get_activations(self,encoding):\n",
    "        # Activation is 1 / Euclidian(vectors, encoding).\n",
    "        # The closer a vector is to the encoding, the higher the activation.\n",
    "        return 1/(self.vectors-encoding).square().sum(dim=-1).sqrt()\n",
    "\n",
    "    def get_bmu(self,encoding):\n",
    "        actvtn = self.get_activations(encoding)\n",
    "        # Especially at the beginning of training, there may be a larger amount\n",
    "        # of vectors that are equidistant to the encoding. \n",
    "        bmu_idxs = (actvtn==torch.max(actvtn)).nonzero()\n",
    "        # In order to prevent embedding collapse, we select one randomly as the bmu.\n",
    "        selected = np.random.randint(low=0,high=len(bmu_idxs))\n",
    "        return bmu_idxs[selected]\n",
    "\n",
    "    def mean_encoding_by_bmu(self,encodings,bmus):\n",
    "        # https://stackoverflow.com/questions/56154604/groupby-aggregate-mean-in-pytorch/56155805#56155805\n",
    "        M = torch.zeros(len(self.vectors), len(encodings))\n",
    "        M[bmus, torch.arange(len(encodings))] = 1\n",
    "        M = torch.nn.functional.normalize(M, p=1, dim=1)\n",
    "        return torch.mm(M, encodings)\n",
    "\n",
    "    def get_distances(self):\n",
    "        # Distance from each node to every other node\n",
    "        xy_dist = self.map_idx.unsqueeze(0) - self.map_idx.unsqueeze(1)\n",
    "        return torch.sqrt(torch.sum(torch.square(xy_dist),dim=-1))\n",
    "\n",
    "    def update_factor(self,bmus):\n",
    "        dists = self.get_distances()\n",
    "        # Gaussian distribution centered on BMU of width 2^gauss.\n",
    "        return torch.exp(torch.neg(torch.div(dists.square(), 2*self.gauss**2)))\n",
    "\n",
    "    def batch_sum_encodings(self,encodings):\n",
    "        # This step is not vectorized, but we could do a random partitioning or something above.\n",
    "        bmus = torch.cat([self.get_bmu(e) for e in encodings])\n",
    "        # Although this is referred to as h_ji in the paper\n",
    "        # it is symmetric (so h[j][i] == h[i][j])\n",
    "        h_ij = self.update_factor(bmus)\n",
    "        x_mj = self.mean_encoding_by_bmu(encodings,bmus)\n",
    "        \n",
    "        bmu_count_by_idx = torch.bincount(bmus, minlength=len(self.map_idx))\n",
    "        # Unsqueeze the first dimension of the counts so that the update factor\n",
    "        # for i to j is weighted based on occurences of j.\n",
    "        weighted_h_ji = bmu_count_by_idx.unsqueeze(0)*h_ij\n",
    "\n",
    "        return torch.mm(weighted_h_ji, x_mj)/weighted_h_ji.sum(dim=-1,keepdim=True)\n",
    "\n",
    "    def unweighted_batch_sum(self,bmus,encodings):\n",
    "        h_ij = self.update_factor(bmus)\n",
    "        x_mj = self.mean_encoding_by_bmu(encodings,bmus)\n",
    "        \n",
    "        bmu_count_by_idx = torch.bincount(bmus, minlength=len(self.map_idx))\n",
    "        # Unsqueeze the first dimension of the counts so that the update factor\n",
    "        # for i to j is weighted based on occurences of j.\n",
    "        weighted_h_ji = bmu_count_by_idx.unsqueeze(0)*h_ij\n",
    "\n",
    "        return torch.mm(weighted_h_ji, x_mj)/weighted_h_ji.sum(dim=-1,keepdim=True)\n",
    "\n",
    "    def update_batch(self,encodings):\n",
    "        self.vectors = self.batch_sum_encodings(encodings)\n",
    "        \n",
    "def test():\n",
    "    mm = SOM(3,2)\n",
    "    encodings = torch.stack([mol[\"encoding\"] for mol in molecules[:10]])\n",
    "    mm.update_batch(encodings)\n",
    "    \n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "48648f62-de8c-4bde-90cb-6151dcf38d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(mtrx):\n",
    "    return mtrx.reshape((mtrx.shape[0]*mtrx.shape[1],-1)).squeeze()\n",
    "\n",
    "class SOM__(object):\n",
    "    def __init__(self,som):\n",
    "        self.vectors = som.vectors\n",
    "        self.map_idx = som.map_idx\n",
    "        self.gauss = som.gauss\n",
    "\n",
    "    def get_activations(self,encoding):\n",
    "        # Activation is 1 / Euclidian(vectors, encoding).\n",
    "        # The closer a vector is to the encoding, the higher the activation.\n",
    "        return 1/(self.vectors-encoding).square().sum(dim=-1).sqrt()\n",
    "\n",
    "    def get_bmu(self,encoding):\n",
    "        actvtn = self.get_activations(encoding)\n",
    "        # Especially at the beginning of training, there may be a larger amount\n",
    "        # of vectors that are equidistant to the encoding. \n",
    "        bmu_idxs = (actvtn==torch.max(actvtn)).nonzero()\n",
    "        # In order to prevent embedding collapse, we select one randomly as the bmu.\n",
    "        selected = np.random.randint(low=0,high=len(bmu_idxs))\n",
    "        return bmu_idxs[selected]\n",
    "\n",
    "    def mean_encoding_by_bmu__(self,encodings,bmus):\n",
    "        sum_mj = torch.zeros(self.vectors.shape)\n",
    "        count_mj = torch.zeros(self.vectors.shape[0])\n",
    "        for i, v_idx in enumerate(bmus):\n",
    "            count_mj[v_idx] += 1\n",
    "            sum_mj[v_idx] += encodings[i]\n",
    "\n",
    "        x_mj = torch.zeros(self.vectors.shape)\n",
    "        for i, sm in enumerate(sum_mj):\n",
    "            if count_mj[i] > 0:\n",
    "                x_mj[i] = sm / count_mj[i]\n",
    "            else:\n",
    "                x_mj[i] = torch.zeros(sm.shape)\n",
    "\n",
    "        return x_mj\n",
    "\n",
    "    def update_factor__(self,bmus):\n",
    "        uf = torch.empty((len(self.map_idx),len(self.map_idx)))\n",
    "        for i, p1 in enumerate(self.map_idx):\n",
    "            for j, p2 in enumerate(self.map_idx):\n",
    "                xy_dist = p1 - p2\n",
    "                d = torch.sqrt(torch.sum(torch.square(xy_dist),dim=-1))\n",
    "                uf[i][j] = torch.exp(torch.neg(torch.div(d.square(), 2*self.gauss**2)))\n",
    "        return uf\n",
    "\n",
    "    def batch_sum_encodings__(self,bmus,encodings):\n",
    "        h_ji = self.update_factor__(bmus)\n",
    "        x_mj = self.mean_encoding_by_bmu__(encodings,bmus)\n",
    "\n",
    "        bmu_count_by_idx = torch.zeros(self.vectors.shape[0])\n",
    "        for i, v_idx in enumerate(bmus):\n",
    "            bmu_count_by_idx[v_idx] += 1\n",
    "\n",
    "        # bse = torch.empty(self.vectors.shape)\n",
    "        # for i in range(len(bse)):\n",
    "        #     denom = torch.empty(bmu_count_by_idx.shape)\n",
    "        #     for j in range(len(bse)):\n",
    "        #         bse[i] += x_mj[j] * h_ji[j][i] * bmu_count_by_idx[j]\n",
    "        #         denom += h_ji[j][i] * bmu_count_by_idx[j]\n",
    "        #     bse[i] = bse[i] / denom[i]\n",
    "\n",
    "        weighted_h_ji = torch.empty(h_ji.shape)\n",
    "        for i in range(len(weighted_h_ji)):\n",
    "            for j in range(len(weighted_h_ji)):\n",
    "                weighted_h_ji[i][j] = h_ji[i][j] * bmu_count_by_idx[j]\n",
    "\n",
    "        bse = torch.zeros(self.vectors.shape)\n",
    "        for i in range(len(weighted_h_ji)):\n",
    "            denom = torch.zeros(bmu_count_by_idx.shape)\n",
    "            for j in range(len(weighted_h_ji)):\n",
    "                bse[i] += x_mj[j] * weighted_h_ji[i][j]\n",
    "                denom[i] += weighted_h_ji[i][j]\n",
    "            \n",
    "            if denom[i] > 0:\n",
    "                bse[i] = bse[i] / denom[i]\n",
    "            else:\n",
    "                bse[i] = torch.zeros(bse[i].shape)\n",
    "\n",
    "\n",
    "        return bse\n",
    "            \n",
    "        \n",
    "def test():\n",
    "    mm = SOM(3,2)\n",
    "    mm__ = SOM__(mm)\n",
    "    encodings = torch.stack([mol[\"encoding\"] for mol in molecules[:10]])\n",
    "    bmus = torch.cat([mm.get_bmu(e) for e in encodings])\n",
    "\n",
    "    assert torch.all(mm.vectors==mm__.vectors)\n",
    "\n",
    "    def test_update_factor():\n",
    "        nonlocal mm, mm__, bmus\n",
    "        h_ji = mm.update_factor(bmus)\n",
    "        h_ji__ = mm__.update_factor__(bmus)\n",
    "        assert torch.isclose(h_ji,h_ji__).all()\n",
    "\n",
    "    def test_update_factor_symmetric():\n",
    "        nonlocal mm, mm__, bmus\n",
    "        h_ji__ = mm__.update_factor__(bmus)\n",
    "        for i in range(len(h_ji__)):\n",
    "            for j in range(len(h_ji__)):\n",
    "                assert h_ji__[i][j] == h_ji__[j][i]\n",
    "            \n",
    "    def test_mean_encoding_by_bmu():\n",
    "        nonlocal mm, mm__, bmus, encodings\n",
    "        x_mj = mm.mean_encoding_by_bmu(encodings,bmus)\n",
    "        x_mj__ = mm__.mean_encoding_by_bmu__(encodings,bmus)\n",
    "        assert torch.isclose(x_mj,x_mj__).all()\n",
    "\n",
    "    def test_batch_sum_encodings():\n",
    "        nonlocal mm, mm__, bmus, encodings\n",
    "        # bmus are selected randomly, so we inject them here\n",
    "        bse = mm.unweighted_batch_sum(bmus,encodings)\n",
    "        bse__ = mm__.batch_sum_encodings__(bmus,encodings)\n",
    "        assert torch.isclose(bse,bse__).all()\n",
    "    \n",
    "    test_update_factor()\n",
    "    test_update_factor_symmetric()\n",
    "    test_mean_encoding_by_bmu()\n",
    "    test_batch_sum_encodings()\n",
    "    \n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1adf8f4c-2146-454c-adef-5e8fe04128b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    mm = SOM(3,2)\n",
    "    encodings = torch.stack([mol[\"encoding\"] for mol in molecules[:10]])\n",
    "    tbmus = torch.cat([mm.get_bmu(e) for e in encodings])\n",
    "    \n",
    "    \n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ee5a4-17dd-45d8-9027-397d206b5d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mols, test_mols = sklearn.model_selection.train_test_split(molecules)\n",
    "\n",
    "def do_train(config,verbose=False):\n",
    "    som = SOM(width=config[\"width\"],\n",
    "              height=config[\"height\"],\n",
    "              gauss=config[\"gauss\"],\n",
    "              decay=config[\"decay\"],\n",
    "              use_onehot=config[\"onehot\"])\n",
    "    \n",
    "    encodings = torch.stack([mol[\"encoding\"] for mol in molecules])\n",
    "        \n",
    "    for _ in tqdm.tqdm(range(config[\"batches\"]),smoothing=0, disable=not verbose):\n",
    "        som.update_batch(encodings)\n",
    "\n",
    "    return som\n",
    "\n",
    "som = do_train({'width': 5, 'height': 4, 'batches': 10,\"gauss\":1, \"decay\": .99, \"onehot\": True},verbose=True)\n",
    "for n, f in all_notes.most_common(10):\n",
    "    print(n,som.get_bmu(multi_hot([n])))\n",
    "print()\n",
    "for mol in molecules[:10]:\n",
    "    print(mol[\"name\"],som.get_bmu(mol[\"encoding\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0ea63-1891-4f3c-abf4-6e004baf447e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619bb584-e314-442b-b116-044981d2c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(mtrx):\n",
    "    return mtrx.reshape((mtrx.shape[0]*mtrx.shape[1],-1)).squeeze()\n",
    "\n",
    "def plot(title,encoding,as_size,factor=1,thresh=1e-5,activations=None):\n",
    "    bmu = som.get_bmu(encoding)\n",
    "    if not torch.is_tensor(activations):\n",
    "        activations = som.get_activations(encoding)\n",
    "    \n",
    "    act = flatten(activations)\n",
    "    minv, maxv = act.min().numpy(), act.max().numpy()\n",
    "    \n",
    "    pos = flatten(som.map_idx).numpy()\n",
    "    \n",
    "    if as_size:\n",
    "        # For very very small values, matplotlib will underflow and draw circles where it should draw tiny circles.\n",
    "        act = torch.nn.functional.threshold(act,thresh,0)\n",
    "        plt.scatter(pos[:,0],pos[:,1],s=factor*act.numpy())\n",
    "    else:\n",
    "        plt.scatter(pos[:,0],pos[:,1],c=factor*act.numpy())\n",
    "        plt.set_cmap('PiYG_r')\n",
    "        plt.colorbar()\n",
    "        # cbar.lim(minv,maxv)\n",
    "\n",
    "    plt.title(f\"{title}\\nBMU of {bmu.numpy()} w/ value = {activations[bmu[0],bmu[1]]}. Range = ({minv:.2f}, {maxv:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebce99a-2fc9-444f-8732-57cf4004129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(f\"Map for {molecules[0]['name']} w/ notes = {molecules[0]['notes']}\",\n",
    "     molecules[0][\"encoding\"],\n",
    "     as_size=True,\n",
    "     factor=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759684e6-c7fd-402f-877a-a86917d70955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(\"\",multi_hot([\"musk\"]),as_size=True,thresh=1e-1)\n",
    "# plot(\"\",multi_hot([\"sweet\"]),as_size=True,thresh=1e-1)\n",
    "# plot(\"\",multi_hot([\"sour\"]),as_size=True,thresh=1e-1)\n",
    "# plot(\"\",multi_hot([\"fruity\"]),as_size=True,thresh=1e-1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf8f70b-20a1-4872-9f01-8954e6f212a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(f\"Update factor for {molecules[0]['name']} w/ notes = {molecules[0]['notes']}\",\n",
    "     molecules[0][\"encoding\"],\n",
    "     as_size=True,\n",
    "     factor=30,\n",
    "     activations= som.update_factor(som.get_bmu(molecules[0][\"encoding\"])))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f54eeb-1504-4ce4-b41c-1cc2fbca24c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, f in all_notes.most_common(100):\n",
    "    # plot(n,multi_hot([n]),as_size=True,thresh=1e-1)\n",
    "    # plt.show()\n",
    "    print(n,som.get_bmu(multi_hot([n])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb0289-f4af-4fa5-82d7-c4fff13df582",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
